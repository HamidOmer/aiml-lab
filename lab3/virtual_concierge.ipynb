{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual Concierge \n",
    "\n",
    "This notebook demonstrates face recogntion with using the [InsightFace](https://github.com/deepinsight/insightface) model on MXNET.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "The following packages need to be installed before proceeding:\n",
    "\n",
    "* MXNet - `pip install mxnet`\n",
    "* numpy - `1pip install numpy`\n",
    "* OpenCV - `pip install opencv-python`\n",
    "* Graphviz - `pip install graphviz`\n",
    "* matplotlib - `pip install matplotlib`\n",
    "* Seaborn `sudo pip3 install seaborn`\n",
    "* Boto3 - `pip install boto3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies\n",
    "\n",
    "Verify that all dependencies are installed using the cell below. Continue if no errors encountered, warnings can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import os\n",
    "from __future__ import print_function\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained model\n",
    "\n",
    "`get_model()` : Loads ONNX model into MXNet symbols and params, defines model using symbol file and binds parameters to the model using params file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(ctx, image_size, model_str, layer):\n",
    "    _vec = model_str.split(',')\n",
    "    assert len(_vec)==2\n",
    "    prefix = _vec[0]\n",
    "    epoch = int(_vec[1])\n",
    "    print('loading',prefix, epoch)\n",
    "    sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)\n",
    "    all_layers = sym.get_internals()\n",
    "    sym = all_layers[layer+'_output']\n",
    "    model = mx.mod.Module(symbol=sym, context=ctx, label_names = None)\n",
    "    model.bind(data_shapes=[('data', (1, 3, image_size[0], image_size[1]))])\n",
    "    model.set_params(arg_params, aux_params)\n",
    "    return model, sym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess images\n",
    "\n",
    "In order to input only face pixels into the network, all input images are passed through a pretrained face detection and alignment model as described above. The output of this model are landmark points and a bounding box corresponding to the face in the image. Using this output, the image is processed using affine transforms to generate the aligned face images which are input to the network. The functions performing this is defined below.\n",
    "\n",
    "`get_input()` : Returns aligned face to the bbox and margin\n",
    "\n",
    "`show_input()` : Shows the image after transposing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input(img, image_size, bbox=None, margin=44):\n",
    "    if bbox is None:\n",
    "        det = np.zeros(4, dtype=np.int32)\n",
    "        det[0] = int(img.shape[1]*0.0625)\n",
    "        det[1] = int(img.shape[0]*0.0625)\n",
    "        det[2] = img.shape[1] - det[0]\n",
    "        det[3] = img.shape[0] - det[1]\n",
    "    else:\n",
    "        det = bbox\n",
    "    bb = np.zeros(4, dtype=np.int32)\n",
    "    bb[0] = np.maximum(det[0]-margin/2, 0)\n",
    "    bb[1] = np.maximum(det[1]-margin/2, 0)\n",
    "    bb[2] = np.minimum(det[2]+margin/2, img.shape[1])\n",
    "    bb[3] = np.minimum(det[3]+margin/2, img.shape[0])\n",
    "    img = img[bb[1]:bb[3],bb[0]:bb[2],:]\n",
    "    img = cv2.resize(img, (image_size[1], image_size[0]))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    aligned = np.transpose(img, (2,0,1))\n",
    "    return aligned\n",
    "\n",
    "def show_input(aligned):\n",
    "    plt.imshow(np.transpose(aligned,(1,2,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Features\n",
    "\n",
    "`l2_normalize()`: Performs row normalization on the vector\n",
    "\n",
    "`get_feature()` : Performs forward pass on the data aligned using model and returns the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_normalize(X):\n",
    "    norms = np.sqrt((X * X).sum(axis=1))\n",
    "    X /= norms[:, np.newaxis]\n",
    "    return X\n",
    "\n",
    "def get_feature(model, aligned):\n",
    "    input_blob = np.expand_dims(aligned, axis=0)\n",
    "    data = mx.nd.array(input_blob)\n",
    "    db = mx.io.DataBatch(data=(data,))\n",
    "    model.forward(db, is_train=False)\n",
    "    embedding = model.get_outputs()[0].asnumpy()\n",
    "    embedding = l2_normalize(embedding).flatten()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Model\n",
    "\n",
    "Load the model on the cpu, then compare badge image images to an aligned image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "image_size = (112,112)\n",
    "model_name = './models/mobilenet1,0'\n",
    "model, sym = get_model(mx.cpu(), image_size, model_name, 'fc1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model\n",
    "mx.viz.plot_network(sym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Load test badge image and compare to uploaded image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('./people/julbrigh.jpg')\n",
    "pre1 = get_input(img1, image_size)\n",
    "show_input(pre1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img1 = cv2.imread('./people/sssalim.jpg')\n",
    "# pre1 = get_input(img1, image_size)\n",
    "# show_input(pre1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = cv2.imread('./people/julbrigh_deeplens.jpg')\n",
    "pre2 = get_input(img2, image_size, bbox=[1368,376,1573,632])\n",
    "show_input(pre2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img2 = cv2.imread('./people/sssalim_deeplens.jpg')\n",
    "# pre2 = get_input(img2, image_size, bbox=[1090,389,1320,663])\n",
    "# show_input(pre2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate predictions\n",
    "\n",
    "Two face images are passed through the network sequentially to generate embedding vectors for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the model \n",
    "out1 = get_feature(model,pre1)\n",
    "out2 = get_feature(model,pre2)\n",
    "out1[:3], out2[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate similarity\n",
    "\n",
    "The squared distance and cosine similarity between the embedding vectors are computed and displayed. Images containing face of a single person will have low distance and high similarity and vice-versa. \n",
    "\n",
    "The distance values are in [0,4) and similarity values in [-1,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute squared distance between embeddings\n",
    "dist = np.sum(np.square(out2-out1))\n",
    "# Compute cosine similarity between embedddings\n",
    "sim = np.dot(out1, out2.T)\n",
    "# Print predictions\n",
    "print('Distance = %f' %(dist))\n",
    "print('Similarity = %f' %(sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Distribution\n",
    "\n",
    "Load the saved vectors for all people in the database, and plot the distribution and outliner for match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "# Load vectors and names\n",
    "people = np.load('./models/people-au.npz')\n",
    "vecs = people['vecs']\n",
    "names = [p.decode('utf-8') for p in people['names']]\n",
    "\n",
    "# calculate cosine similarity and relative zscores\n",
    "sims = np.dot(vecs, out2)\n",
    "zscores = stats.zscore(sims)\n",
    "\n",
    "# plot series and print score and name\n",
    "sns.set(color_codes=True)\n",
    "plt.subplots(figsize=(10,6))\n",
    "ax = sns.distplot(zscores, bins=50, kde=False, rug=True)\n",
    "ax.set(xlabel='zscore', ylabel='number of people')\n",
    "plt.title('zscore distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = sims.argmax()\n",
    "print('sim: {}, zscore: {}, name: {}'.format(sims[idx], zscores[idx], names[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
